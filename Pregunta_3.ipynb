{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pregunta 3","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0Y1tHj3DNqhX"},"source":["<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n","\n","\n","<hr style=\"height:2px;border:none\"/>\n","<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n","\n","<H3 align='center'> Tarea 2 - Aplicaciones Recientes de Redes Neuronales </H3>\n","\n","<H2 align='center'> Pregunta 3. Encoder-Decoder sobre texto</H2>\n","<H3 align='center'> Francisca Ramírez</H3>\n","<H3 align='center'> Sebastian Ramírez</H3>\n","\n","<hr style=\"height:2px;border:none\"/>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"K4UIcvJnLZaZ","colab_type":"text"},"source":["<a id=\"tercero\"></a>\n","## 3. *Encoder-Decoder* sobre Texto\n","\n","Trabajos recientes en redes neuronales han demostrado que se puede aplicar a problemas bastante complejos gracias a la flexibilidad la definición de las redes, además de que se pueden adaptar a distintos tipos de datos brutos (dominios). Con el objetivo de explorar el enfoque anterior de *traducción* de algun tipo de dato, en esta sección deberá realizarlo con texto para traducción de un lenguaje humano a otro (e.g. inglés a alemán, chino a ruso).\n","\n","<img src=\"https://www.panoramaila.cz/images/preklady.jpg\" width=\"35%\" />\n","\n","\n","Trabajaremos con el dataset de pares de sentencias bilingues entre distintos idiomas del proyecto __[Tatoeba](http://www.manythings.org/anki/)__. El objetivo entonces consta de tomar un texto en lenguaje natural de algún idioma (*source*) y traducirlo a otro texto en lenguaje natural de otro idioma (*target*), donde cada texto tendrá un largo variable. Lo cual se empleará a través de extrar información del texto *source* (*encoder*) para luego generar el texto *target* (*decoder*) en base a esta información extraída.\n","\n","\n","Deberá seleccionar el *dataset* que guste para trabajar con la tarea de traducción, comente sobre su decisión. Luego cárgelo con *pandas*.\n","```python\n","import pandas as pd\n","df = pd.read_csv(\"data/dataset_selected.txt\", sep=\"\\t\", names=[\"Source\",\"Target\"])\n","df.head()\n","```\n","\n","> a) Visualice los datos ¿Qué es la entrada y qué es la salida? Comente sobre los múltiples significados/sinónimos que puede tener una palabra al ser traducida y cómo propondría arreglar eso. *se espera que pueda implementarlo*\n","\n","> b) Realice un pre-procesamiento a los textos como se acostumbra para eliminar símbolos inecesarios u otras cosas que estime conveniente, comente sobre la importancia de éste paso. Además de ésto deberá agregar un símbolo al final de la sentencia *target* para indicar un \"alto\" cuando la red neuronal necesite aprender a generar una sentencia.\n","```python\n","import string\n","table = str.maketrans('', '', string.punctuation) \n","def clean_text(text, where=None):\n","    \"\"\" OJO: Sin eliminar el significado de las palabras.\"\"\"\n","    text = text.lower()\n","    tokenize_text = text.split()\n","    tokenize_text = [word.translate(table) for word in tokenize_text]#eliminar puntuacion\n","    tokenize_text = [word for word in tokenize_text if word.isalpha()] #remove numbers\n","    if where ==\"target\":\n","        tokenize_text = tokenize_text + [\"#end\"] \n","    return tokenize_text\n","texts_input = list(df['Source'].apply(clean_text))\n","texts_output = list(df['Target'].apply(clean_text, where='target'))\n","```\n","\n","> Cree un conjunto de validación y de pruebas fijos de $N_{exp} = 10000$ datos ¿Cuántos datos quedan para entrenar? \n","```python\n","from sklearn.model_selection import train_test_split\n","X_train_l, X_test_l, Y_train_l, Y_test_l = train_test_split(texts_input, texts_output,\n","                                                            test_size=N_exp, random_state=22)\n","X_train_l, X_val_l, Y_train_l, Y_val_l = train_test_split(X_train_l, Y_train_l, \n","                                                          test_size=N_exp, random_state=22)\n","```\n","*Recuerde que si no puede procesar los datos de entrenamiento adecuadamente siempre puede muestrear en base a la capacidad de cómputo que posea*\n","\n","> c) Genere un vocabulario, **desde el conjunto de entrenamiento**, sobre las palabras a recibir y generar en la traducción, esto es codificarlas a un valor entero que servirá para que la red las vea en una representación útil a procesar, *comience desde el 1 debido a que el cero será utilizado más adelante*. Para reducir el vocabulario considere las palabras que aparecen un mínimo de *min_count* veces en todo los datos, se aconseja un valor de 3. Comente sobre la importancia de ésto al reducir el vocabulario ¿De qué tamaño es el vocabulario de entrada y salida? ¿La diferencia de ésto podría ser un factor importante?\n","```python\n","def create_vocab(texts, min_count=1):\n","    count_vocab = {}\n","    for sentence in texts:\n","        for word in sentence:\n","            if word not in count_vocab:\n","                count_vocab[word] = 1\n","            else:\n","                count_vocab[word] += 1\n","    return [word for word,count in count_vocab.items() if count >= min_count]\n","vocab_source = create_vocab(X_train_l, min_count=3)\n","word2idx_s = {w: i+1 for i, w in enumerate(vocab_source)} #index (i+1) start from 1,2,3,...\n","idx2word_s = {i+1: w for i, w in enumerate(vocab_source)}\n","n_words_s = len(vocab_source)\n","vocab_target = create_vocab(Y_train_l, min_count=3)\n","word2idx_t = {w: i+1 for i, w in enumerate(vocab_target)}  #Converting text to numbers\n","idx2word_t = ... #Converting number to text\n","n_words_t = ...\n","```\n","Ahora codifique las palabras a los números indexados con el vocabulario. Recuerde que si una palabra en los otros conjuntos, o en el mismo de entrenamiento, no aparece en el vocabulario no se podrá generar una codificación, por lo que será **ignorada** ¿Cómo se podría evitar ésto?\n","```python\n","\"\"\" Source/input data \"\"\"\n","dataX_train = [[word2idx_s[word] for word in sent if word in word2idx_s] for sent in X_train_l]\n","dataX_valid = [[word2idx_s[word] for word in sent if word in word2idx_s] for sent in X_val_l]\n","... #do test\n","\"\"\" Target/output data \"\"\"\n","dataY_train = [[word2idx_t[word] for word in sent if word in word2idx_t] for sent in Y_train_l]\n","dataY_valid = [[word2idx_t[word] for word in sent if word in word2idx_t] for sent in Y_val_l] \n","...#do test\n","```\n","\n","> d) Debido al largo variable de los textos de entrada y salida será necesario estandarizar ésto para poder trabajar de manera más cómoda en Keras, *cada texto (entrada y salida) pueden tener distinto largo máximo*. Comente sobre la decisión del tipo de *padding*, *pre o post* ¿Qué sucede al variar el largo máximo de instantes de tiempo para procesar en cada parte del modelo (entrada y salida)?\n","```python\n","from keras.preprocessing import sequence\n","\"\"\" INPUT DATA (Origin language) \"\"\"\n","max_inp_length = max(map(len,dataX_train))\n","print(\"Largo max inp: \",max_inp_length)\n","word2idx_s[\"*\"] = 0 #padding symbol\n","idx2word_s[0] = \"*\"\n","n_words_s += 1  \n","X_train = sequence.pad_sequences(dataX_train, maxlen=max_inp_length, padding='pre', value=word2idx_s[\"*\"])\n","...# do valid and test..\n","\"\"\" OUTPUT DATA (Destination language) \"\"\"\n","max_out_length = max(map(len,dataY_train)) \n","print(\"Largo max out: \",max_out_length)\n","word2idx_t[\"*\"] = 0 #padding symbol\n","idx2word_t[0] = \"*\"\n","n_words_t += 1  \n","Y_train = sequence.pad_sequences(dataY_train, maxlen=max_out_length, padding='post', value=word2idx_t[\"*\"])\n","...#do valid and test..\n","```\n","\n","> e) Para evitar que la red obtenga una ganancia por imitar/predecir el símbolo de *padding* que está bastante presente en los datos coloque un peso sobre éste clase, con valor 0, así se evita que tenga impacto en la función objetivo. Ya que *keras* no soporta directamente ésto en series de tiempo coloque el peso a cada instante de tiempo de cada dato de entrenamiento dependiendo de su clase. Comente sobre alguna otra forma en que se podría manejar el evitar que la red prediga en mayoría el símbolo de *padding*.\n","```python\n","c_weights = np.ones(n_words_t)\n","c_weights[0] = 0 #padding class masked\n","sample_weight = np.zeros(Y_train.shape)\n","for i in range(sample_weight.shape[0]):\n","    sample_weight[i] = c_weights[Y_train[i,:]]\n","```\n","\n","> f) Para lograr la tarea defina una red recurrente del tipo *encoder*-*decoder* como la que se presenta en la siguiente imágen.\n","<img src=\"https://chunml.github.io/ChunML.github.io/images/projects/sequence-to-sequence/repeated_vector.png\" width=\"60%\" />\n","En primer lugar defina el *Encoder* que procesara el texto de entrada y retornará un solo vector final, haciendo uso de las capas ya conocidas de *Embedding* para generar un vector denso de palabra y *GRU*, pero en su versión acelerada para GPU.\n","```python\n","from keras.models import Sequential\n","from keras.layers import Embedding,CuDNNGRU\n","EMBEDDING_DIM = 100\n","model = Sequential()\n","model.add(Embedding(input_dim=n_words_s, output_dim=EMBEDDING_DIM, input_length=max_inp_length))\n","model.add(CuDNNGRU(64, return_sequences=True))\n","model.add(CuDNNGRU(128, return_sequences=False))\n","```\n","Luego defina la sección que conecta el largo (*timesteps*) de entrada *vs* el de salida.\n","```python\n","from keras.layers import RepeatVector\n","model.add(RepeatVector(max_out_length)) #conection\n","```\n","Finalmente defina el *Decoder* para generar la secuencia de salida en texto de palabras en otro idioma, a través de la función *softmax* sobre cada instante de tiempo (*timestep*. \n","```python\n","from keras.layers import CuDNNGRU, TimeDistributed,Dense\n","model.add(CuDNNGRU(128, return_sequences=True))\n","model.add(CuDNNGRU(64, return_sequences=True))\n","model.add(TimeDistributed(Dense(n_words_t, activation='softmax')))\n","model.summary()\n","```\n","Entrene la red entre 1 a 5 *epochs*, agregando los pesos definidos sobre cada ejemplo de entrenamiento. Además de utilizar una función de pérdida que evita generar explícitamente los *one hot vector*\n","```python\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', sample_weight_mode='temporal')\n","model.fit(X_train, Y_train, epochs=3, batch_size=256,validation_data=(X_valid, Y_valid),\n","         sample_weight = sample_weight) \n","```\n","\n","> g) Debido a lo costoso de tener una red completamente recurrente para entrenar y poder experimentar, cambie el modelo que procesa el *Encoder* por una red convolucional, reduciendo el número de capas pero aumentando las neuronas. Utilice tamaños de *kernel*  igual a 5 y funciones de activaciones relu. Se agregan capas de *BatchNormalization* debido a que en el *Decoder* contamos con redes recurrentes que tienen capa activación distinta a la usada por las convoluciones. La capa de *GlobalMaxPooling1d* es lo que permite reducir toda la información extraída a un único vector, como se realizó anteriormente con *return_sequences=False*, comente sobre la ganancia o desventaja de ésto *vs* la red neuronal.\n","```python\n","from keras.layers import Conv1D,MaxPool1D,GlobalMaxPooling1D,GlobalAveragePooling1D,BatchNormalization\n","model = Sequential()\n","model.add(Embedding(input_dim=n_words_s, output_dim=EMBEDDING_DIM, input_length=max_inp_length))\n","model.add(Conv1D(256, 5, padding='same', activation='relu', strides=1))\n","model.add(BatchNormalization()) #for stability\n","model.add(Conv1D(256, 5, padding='same', activation='relu', strides=1))\n","model.add(BatchNormalization())\n","model.add(GlobalMaxPooling1D()) #aka to return_sequences=False\n","model.add(RepeatVector(max_out_length)) #conection\n","model.add(CuDNNGRU(256, return_sequences=True))\n","model.add(TimeDistributed(Dense(n_words_t, activation='softmax')))\n","model.summary() \n","```\n","Entrene el modelo igual a lo presentado anteriormente pero ahora por 20 *epochs* ¿Cambian los tiempos de procesamiento y la cantidad de parámetros?\n","\n","> h) Visualice lo aprendido por el modelo sobre algunos datos del conjunto de entrenamiento y validación, comente lo observado.\n","```python\n","def predict_words(y_indexs, data=\"target\"):\n","    \"\"\" Predict until '-#end-' is seen \"\"\"\n","    return_val = []\n","    for indx_word in y_indexs:\n","        if indx_word != 0: #start to predict\n","            return_val.append(np.squeeze(indx_word))\n","            if data == \"target\": #if target is predicting\n","                if indx_word == word2idx_t[\"#end\"]:\n","                    return return_val                \n","    return return_val\n","n_s = 100\n","idx = np.random.choice(np.arange(Y_set.shape[0]), size=n_s, replace=False)\n","Y_set_pred = model.predict_classes(X_set[idx] )\n","for i, n_sampled in enumerate(idx):\n","    text_input = [idx2word_s[p] for p in predict_words(X_set[n_sampled], data=\"source\")]\n","    print(\"Texto source: \", ' '.join(text_input))\n","    text_real = [idx2word_t[p] for p in predict_words(Y_set[n_sampled,:,0], data=\"target\")]\n","    print(\"Texto target real: \", ' '.join( text_real))\n","    text_sampled = [idx2word_t[p] for p in predict_words(Y_set_pred[i], data=\"target\")]\n","    print(\"Texto target predicho: \", ' '.join(text_sampled))\n","```\n","\n","> i) Realice algún cambio esperando que mejore el modelo entrenado, luego vuelva a visualizar lo predicho por la red *vs* lo real. *Debido a lo costoso en entrenar puede optar por realizar solo un cambio pero que sea significativo*.  Se comentan algunas opciones para utilizar y combinar:\n","* Cambiar  el *embedding* por alguno pre-entrenado\n","* Agregar regularizadores\n","* Asignar peso a las clases/palabras de salida\n","* Cambiar *Global max pooling* por *Average max pooling*\n","* Aumentar o reducir capas\n","* Aumentar o reducir neuronas/unidades  \n","\n","> j) A pesar de que la tarea de medir qué tan similar es un texto a otro ya es un área de investigación propia [[6]](#refs), usted deberá utilizar alguna métrica de desempeño para ver qué tan buena es la traducción del texto *versus* el texto real entregado. Debido a que la métrica de *Exact Matching* (EM) puede ser muy drástica, mida *f1 score* por texto además de proponer alguna otra técnica de evaluación para medir sobre el conjunto de pruebas y los otros conjuntos si estima conveniente. Puede basarse en otros trabajos como *Image captioning* o *Text summary*. \n","*Hint: Debido a los problemas de memoria al realizar un forward-pass, solo seleccione un subconjunto $N_{sub}$ del conjunto de pruebas para realizar ésta evaluación, se aconseja entre 1000 y 5000.*\n","```python\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","m = MultiLabelBinarizer().fit([np.arange(n_words_t)]) \n","def calculate_f1(true, pred):\n","    true = np.squeeze(true)\n","    pred = np.squeeze(pred)\n","    binarized_true = m.transform([predict_words(true)])[0] #onehot of words appear\n","    binarized_pred = m.transform([predict_words(pred)])[0] #onehot of words appear\n","    return f1_score(binarized_true, binarized_pred, average='binary') #only on appearing words\n","f1_final = np.mean([calculate_f1(true_words,pred_words) for true_words,pred_words in zip(Y_true,Y_hat)])\n","f1_final*100 #porcentaje\n","```\n",">> La función de *f1 score* en este extracto se calcula en base al *precision* y *recall* de que aparezca cada una de las palabras predichas dentro de las palabras reales (como si cada palabra fuera una clase de \"aparece\" o no), **sin importar el orden ni la ocurrencia**.\n","\n","\n","> k) En ves de volver a variar el modelo de *Encoder*, dejaremos una representación manual explícita (*no entrenable*) a través de extraer características manuales de los textos *source*, como por ejemplo representaciones *term frequency* (TF) o TF-IDF, proporcionadas a través de __[sklearn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text)__. Luego, con esto generado, defina y entrene el modelo *Decoder* neuronal como el presentado en las preguntas anteriores, ésto es comenzar desde la capa *RepeatVector* hasta llegar a la clasificación sobre el texto *target*. Compare el desempeño con lo presentado en (j) y lo visualizado en (h).\n","```python\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","def dummy_fun(doc):\n","    return doc\n","tf_idf = TfidfVectorizer(analyzer='word',tokenizer=dummy_fun,preprocessor=dummy_fun,\n","                         token_pattern=None,use_idf= True, smooth_idf=True, norm='l2')   \n","X_train_tfidf = tf_idf.fit_transform(dataX_train).astype('float32').todense()\n","...#do over valid and test..\n","```"]}]}
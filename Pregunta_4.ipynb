{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Enunciado_T2.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0Y1tHj3DNqhX"},"source":["<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n","\n","\n","<hr style=\"height:2px;border:none\"/>\n","<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n","\n","<H3 align='center'> Tarea 2 - Aplicaciones Recientes de Redes Neuronales </H3>\n","\n","<H2 align='center'> Pregunta 4. </H2>\n","<H3 align='center'> Francisca Ramírez</H3>\n","<H3 align='center'> Sebastian Ramírez</H3>\n","\n","<hr style=\"height:2px;border:none\"/>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Qwl60J5mL3Ml","colab_type":"text"},"source":["<a id=\"cuarto\"></a>\n","## 4. Distintos tipos de autoencoders (AEs) en MNIST\n","---\n","\n","Como se ha discutido en clases, las RBM’s y posteriormente los AE's (redes no supervisadas) fueron un componente crucial en el desarrollo de los modelos que entre 2006 y 2010 vigorizaron el área de las redes neuronales artificiales con logros notables de desempeño en diferentes tareas de aprendizaje automático. Recientemente se ha propuesto AE's con distribuciones de probabilidades en su codificación, VAE [[7]](#refs). Los VAE son una variación bayesiana que aprende los parámetros de alguna distribución de probabilidad de variables latentes definida sobre los datos, a través de esa variable latente el decodificador generar/reconstruye nuevos datos $\\hat{x}$. En resumidas cuentas, es un autoencoder que aprende un modelo sobre las variables latentes de los datos.\n","\n","<img src=\"https://www.researchgate.net/profile/Steven_Young11/publication/306056875/figure/fig1/AS:393921575309346@1470929630835/Example-images-from-the-MNIST-dataset.png\" title=\"mnist\" width=\"25%\" style=\"float: right;\" />\n","\n","Con este objetivo en mente, utilizaremos un dataset simple denominado **MNIST**. Se trata de una colección de 70000 imágenes de 28 $\\times$ 28 pixeles correspondientes a dígitos manuscritos (números entre 0 y 9). En su versión tradicional, la colección se encuentra separada en dos subconjuntos: uno de entrenamiento de 60000 imágenes y otro de test de 10000 imágenes\n","\n","\n","\n","Cargue los datos desde el repositorio de Keras.\n","```python\n","import numpy as np\n","import keras\n","import keras.backend as K\n","from keras.datasets import mnist\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train[:,:,:,None] #add channels\n","x_test = x_test[:,:,:,None]\n","img_rows, img_cols,channel = x_train.shape[1:]\n","original_img_size = (img_rows, img_cols,channel) # input image dimensions\n","```\n","\n","> a) Normalice las imágenes de modo que los pixeles queden en el rango [0, 1] como se acostumbra al trabajar con imágenes. Visualice y comente sobre los datos a trabajar.\n","```python\n","x_train = x_train.astype('float32') / 255. \n","...#and x_test\n","```\n","\n","### 4.1 Autoencoder clásico\n","Una de las aplicaciones tı́picas de un AE es **reducción de dimensionalidad**, es decir, implementar una transformación $\\phi:{\\rm I\\!R}^d \\rightarrow {\\rm I\\!R}^{d'}$ de objetos representados originalmente por $d$ atributos en una nueva representación de $d'$ atributos, de modo tal que se preserve lo mejor posible la “información” original. Obtener tal representación es útil desde un punto de vista computacional (compresión) y estadı́stico (permite construir modelos con un menor número de parámetros libres). Un AE es una técnica de reducción de dimensionalidad no supervisada porque no hace uso de información acerca de las clases a las que pertenecen los datos de entrenamiento.\n","\n","\n","<img src=\"https://miro.medium.com/max/1400/0*yGqTBMopqHbR0fcF.\" title=\"AE\" width=\"50%\" />\n","\n","\n","> a) Entrene un AE básico, 1 capa escondida *feed forward*, para generar una representación de MNIST en $d'= 2, 8, 16, 32$ dimensiones. Justifique la elección de la función de pérdida a utilizar y del criterio de entrenamiento en general. Determine el porcentaje de compresión obtenido y el error de reconstrucción en cada caso. ¿Mejora el resultado si elegimos una función de activación **ReLU** para el Encoder? ¿Podrı́a utilizarse esta activación en el Decoder?\n","```python\n","from keras.layers import Input, Dense, Flatten,Reshape\n","from keras.models import Model\n","compres_dim = 2\n","input_img = Input(shape=original_img_size)\n","input_fl = Flatten()(input_img) #to get a vector representation\n","encoded = Dense(compres_dim, activation='sigmoid')(input_fl)\n","decoded = Dense(np.prod(original_img_size), activation='sigmoid')(encoded)\n","decoded = Reshape(original_img_size)(decoded)\n","autoencoder = Model(inputs=input_img, outputs=decoded)\n","encoder = Model(inputs=input_img, outputs=encoded)\n","autoencoder.compile(optimizer='rmsprop', loss='binary_crossentropy')\n","autoencoder.fit(x_train,x_train,epochs=40,batch_size=32,validation_data=(x_test,x_test))\n","autoencoder.save('basic_autoencoder.h5')\n","```\n","\n","> b) Compare visualmente la reconstrucción que logra hacer el autoencoder desde la representación en ${\\rm I\\!R}^{d'}$ para algunas imágenes del conjunto de pruebas. Determine si la percepción visual se corresponde con el error de reconstrucción observada. Comente.\n","```python\n","from keras.models import load_model\n","autoencoder = load_model('basic_autoencoder.h5')\n","decoded_test = autoencoder.predict(x_test)\n","encoded_test = encoder.predict(x_test)\n","import matplotlib.pyplot as plt\n","n = 10\n","plt.figure(figsize=(20, 4))\n","for i in range(n):\n","    ax = plt.subplot(2, n, i + 1)\n","    plt.imshow(x_test[i].reshape(28, 28),cmap='gray')\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","    ax = plt.subplot(2, n, i + 1 + n)\n","    plt.imshow(decoded_test[i].reshape(28, 28),cmap='gray')\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","plt.show()\n","```\n","\n","> c) Para verificar la calidad de la representación obtenida, implemente el clasificador denominado $kNN$ (k-nearest neighbor): dada una imagen $x$, el clasificador busca las k = 10 imágenes de entrenamiento más similares (de acuerdo a una distancia, e.g. euclidiana) y predice como clase, la etiqueta más popular entre las imágenes cercanas. Mida el error, en conjunto de entrenamiento y pruebas, obtenido construyendo este clasificador sobre la data reducida a través del autoencoder comparando con la representación reducida obtenida vía PCA (una técnica clásica de reducción de dimensionalidad) utilizando el mismo número de dimensiones $d'$= 2, 8, 16, 32. Considere tanto el error de reconstrucción como el desempeño en clasificación, además de comparar los tiempos medios de predicción en ambos escenarios. \n","```python\n","from sklearn.decomposition import PCA\n","from sklearn.neighbors import KNeighborsClassifier\n","pca = PCA(n_components=compres_dim)\n","pca.fit(x_train)\n","pca_train = pca.transform(x_train)\n","pca_test = pca.transform(x_test)\n","encoded_train = encoder.predict(x_train)\n","encoded_test = encoder.predict(x_test)\n","clf = KNeighborsClassifier(10) #CLASIFICATION\n","clf.fit(pca_train, y_train)\n","print('Classification Accuracy PCA %.2f' % clf.score(pca_test,y_test))\n","clf = KNeighborsClassifier(10) #CLASIFICATION\n","clf.fit(encoded_train, y_train)\n","print('Classification Accuracy %.2f' % clf.score(encoded_test,y_test))\n","```\n","\n","\n","> d) Modifique el autoencoder básico construido en (a) para implementar un *deep autoencoder* (más de dos capas) haciendo uso de las capas convolucionales para trabajar sobre matrices. Comente cómo sufre las transformaciones el patrón de entrada. Demuestre experimentalmente que este autoencoder puede mejorar la compresión obtenida por PCA y por el obtenido en (a) utilizando el mismo número de dimensiones $d'$ . Considere en esta comparación tanto el error de reconstrucción como el desempeño en clasificación (vı́a kNN) de cada representación. Comente.\n","```python\n","from keras.layers import Input,Conv2D,Flatten,Dense,MaxPooling2D, UpSampling2D\n","latent_dim = 2\n","input_img = Input(shape=original_img_size)\n","x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n","x = MaxPooling2D((2, 2))(x)\n","x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n","x = MaxPooling2D((2, 2))(x)\n","before_F_shape =  (x.shape[1].value, x.shape[2].value, x.shape[3].value)\n","x = Flatten()(x)\n","encoded = Dense(compres_dim, activation=...)(x)\n","x = Dense(np.prod(before_F_shape),activation='relu')(encoded)\n","x = Reshape(before_F_shape)(x)\n","x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n","x = UpSampling2D((2, 2))(x)\n","x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n","x = UpSampling2D((2, 2))(x)\n","decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n","autoencoder = Model(input_img, decoded)\n","autoencoder.compile(optimizer='rmsprop', loss='binary_crossentropy')\n","autoencoder.summary()\n","autoencoder.fit(x_train,x_train,epochs=40,batch_size=32,validation_data=(x_test,x_test))\n","autoencoder.save('deep_autoencoder.h5')\n","from sklearn.decomposition import PCA\n","from sklearn.neighbors import KNeighborsClassifier\n","pca = PCA(n_components=target_dim)\n","pca.fit(x_train)\n","```\n","\n","\n","> e) Elija algunas de las representaciones aprendidas anteriormente y visualı́celas usando la herramienta *TSNE* disponible en la librerı́a *sklearn*. Compare cualitativamente el resultado con aquel obtenido usando PCA con el mismo número de componentes.\n","```python\n","nplot=5000 #warning: mind your memory!\n","encoded_train = encoder.predict(x_train[:nplot])\n","from sklearn.manifold import TSNE\n","model = TSNE(n_components=2, random_state=0)\n","encoded_train = model.fit_transform(encoded_train)\n","plt.figure(figsize=(10, 10))\n","colors={0:'b',1:'g',2:'r',3:'c',4:'m',5:'y',6:'k',7:'orange',8:'darkgreen',9:'maroon'}\n","markers={0:'o',1:'+',2: 'v',3:'<',4:'>',5:'^',6:'s',7:'p',8:'*',9:'x'}\n","for idx in xrange(0,nplot):\n","    label = y_train[idx]\n","    line = plt.plot(encoded_train[idx][0], encoded_train[idx][1],\n","        color=colors[label], marker=markers[label], markersize=6)\n","pca_train = pca.transform(x_train)\n","encoded_train = pca_train[:nplot]\n","... #plot PCA\n","```\n","\n","### 4.2 Variational Autoencoder tradicional\n","El enfoque optimizador de los VAE sobre los parámetros modelados $\\theta$ (decoder) y $\\phi$ (encoder) que se deriva consta de minimizar la reconstrucción de los datos (al igual que un autoencoder tradicional), en base a alguna medicicón de error (*mse* por ejemplo) además de tener un factor de regularización que se impone para que la distribución aprendida de las variables latentes sea similar alguna distribución deseada *a priori*.  \n","\n","$$ Min \\ \\mathcal{L}(p_{\\theta}(x\\mid z), \\ x)\\ +\\ KL( q_{\\phi}(z\\mid x) \\mid \\mid p_{\\theta}(z))$$\n","\n","Con $\\mathcal{L}$ la función de pérdida de reconstrucción, $KL$ la *KL Divergence* [[8]](#refs), $q_{\\phi}(z \\mid x)$ la codificación del dato a la variable latente $z$, $p_{\\theta}(x\\mid z)$ la recontrucción de los datos a través de las variables latentes $z$ y  $p_{\\theta}(z)$ una distribución *a priori* asignada. \n","\n","<img src=\"https://i.imgur.com/ZN6MyTx.png\" title=\"VAE\" width=\"60%\" />\n","\n","> a) Defina la sección del *encoder* del VAE como el que se muestra en el código, de 3 tandas convolucionales y una *fully conected*, con una distribución Normal de 2 componentes para las variables latentes, $z \\sim \\mathcal{N} (\\mu, \\sigma^2 )$. Describa la arquitectura utilizada.\n","```python\n","from keras.layers import Input,Conv2D,Flatten,Dense,MaxPool2D\n","from keras.models import Model\n","filters = 32 # number of convolutional filters to use\n","num_conv = 3 # convolution kernel size\n","intermediate_dim = 128\n","latent_dim = 2\n","x = Input(shape=original_img_size)\n","conv_1 = Conv2D(filters,kernel_size=num_conv,padding='same', activation='relu')(x)\n","conv_2 = Conv2D(filters,kernel_size=num_conv,padding='same', activation='relu')(conv_1)\n","conv_3 = Conv2D(filters*2, kernel_size=num_conv, padding='same', activation='relu', strides=2)(conv_2)\n","flat = Flatten()(conv_3)\n","hidden = Dense(intermediate_dim, activation='relu')(flat)\n","z_mean = Dense(latent_dim,activation='linear')(hidden)\n","z_log_var = Dense(latent_dim,activation='linear')(hidden)\n","encoder = Model(x, z_mean) # build a model to project inputs on the latent space\n","```\n","\n","> b) Defina la sección del *decoder* del VAE como el que se muestra en el código, una tanda *fully conected* y 3 tandas de la operación inversa a una convolución (**Convolución transpuesta** [[9]](#refs)), comente cómo ésta trabaja y cómo funcionan los parámetros de *stride*.\n","```python\n","from keras.layers import Reshape,Conv2DTranspose,Activation\n","shape_before_flattening = K.int_shape(conv_3)[1:] # we instantiate these layers separately to reuse them later\n","decoder_hid = Dense(intermediate_dim, activation='relu')\n","decoder_upsample = Dense(np.prod(shape_before_flattening), activation='relu')\n","decoder_reshape = Reshape(shape_before_flattening)\n","decoder_deconv_1 = Conv2DTranspose(filters,kernel_size=num_conv, padding='same',strides=2,activation='relu')\n","decoder_deconv_2 = Conv2DTranspose(filters,kernel_size=num_conv,padding='same', activation='relu')\n","decoder_mean_squash = Conv2DTranspose(channel, kernel_size=num_conv,padding='same', activation='sigmoid')\n","```\n","\n","> c) Defina la sección que conecta a estas dos partes a través de un muestreo explícito de la distribución Normal (con $\\epsilon \\sim \\mathcal{N}(0,1)$ se tiene $g = \\mu + \\sigma \\cdot \\epsilon$), ésto es lo que lo hace que sea un enfoque probabilístico/bayesiano. Describa el modelo completo.\n","```python\n","def sampling(args):\n","    epsilon_std = 1.0\n","    z_mean, z_log_var = args\n","    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),mean=0., stddev=epsilon_std)\n","    return z_mean + K.exp(z_log_var) * epsilon\n","from keras.layers import Lambda\n","z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n","hid_decoded = decoder_hid(z)\n","up_decoded = decoder_upsample(hid_decoded)\n","reshape_decoded =  decoder_reshape(up_decoded)\n","deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n","x_decoded_relu = decoder_deconv_2(deconv_1_decoded)\n","x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n","vae_norm = Model(x, x_decoded_mean_squash) # instantiate VAE model\n","vae_norm.summary()\n","```\n","\n","> d) Como la función objetivo es *customizada* deberemos definirla y poner una distribución a *priori* sobre las variables latentes, en este caso se tendrá como media un vector de ceros y la matriz de covarianza la matriz identidad $p_{\\theta}(z) \\sim N (\\vec{0},I)$. Elija la función de pérdida para la reconstrucción. Comente porqué la *KL Divergence* podría funcionar como regularizador del criterio de entrenamiento obtenido.\n","```python\n","from keras import backend as K # Compute VAE loss\n","choised_loss =  keras.metrics.binary_crossentropy(K.flatten(x),K.flatten(x_decoded_mean_squash))\n","choised_loss =  keras.metrics.mean_squared_error(K.flatten(x),K.flatten(x_decoded_mean_squash))\n","reconstruction_loss = img_rows * img_cols * channel* choised_loss\n","kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1) #closed form\n","vae_loss = K.mean(reconstruction_loss + kl_loss)\n","vae_norm.add_loss(vae_loss)\n","vae_norm.summary()\n","```\n","\n","> e) Entrene el modelo definido con los datos de MNIST entre 20 a 30 *epochs* con el optimizador de *RMSprop* y tamaño de batch el que estime conveniente.\n","```python\n","batch_size = ...\n","epochs =  [20,30]\n","vae_norm.compile(optimizer='rmsprop')\n","vae_norm.fit(x_train,epochs=epochs, batch_size=batch_size,validation_data=(x_test, None))\n","```\n","\n","> f) Visualice la representación codificada $z$ (variables latentes) de los datos en base a su media $\\mu_i$, compare cualitativamente con la representación *TSNE* del AE tradicional. Además genere un histograma de la media y la varianza $\\sigma_i^2$ de las dos componentes. Comente.\n","```python\n","x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n","import matplotlib.pyplot as plt\n","plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n","plt.colorbar()\n","plt.show() # display a 2D plot of the digit classes in the latent space\n","encoder_log_var = Model(x,z_log_var)\n","...#histogram\n","```\n","\n","> g) Genere nuevos datos artificialmente a través del espacio de las variables latentes. Para esto deberá generar puntos linealmente separados por debajo de la distribución Normal. Comente qué significada cada eje en la imagen ¿Qué sucede más allá en el espacio del 90% confianza de las variables latentes? ¿Qué objetos se generan?\n","```python\n","decoder_input = Input(shape=(latent_dim,))\n","_hid_decoded = decoder_hid(decoder_input)\n","_up_decoded = decoder_upsample(_hid_decoded)\n","_reshape_decoded = decoder_reshape(_up_decoded)\n","_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n","_x_decoded_relu = decoder_deconv_2(_deconv_1_decoded)\n","_x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n","generator = Model(decoder_input, _x_decoded_mean_squash) \n","n = 30  # figure with 15x15 images \n","image_size = img_cols\n","figure = np.zeros((image_size * n, image_size * n))\n","from scipy.stats import norm\n","grid_x = norm.ppf(np.linspace(0.05, 0.95, n)) #metodo de la transformada inversa\n","grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n","for i, yi in enumerate(grid_x):\n","    for j, xi in enumerate(grid_y):\n","        z_sample = np.array([[xi, yi]])            \n","        x_decoded = generator.predict(z_sample,batch_size=batch_size)\n","        figure[i * image_size: (i + 1) * image_size,\n","               j * image_size: (j + 1) * image_size] = x_decoded[0][:,:,0]\n","plt.figure(figsize=(10, 10))\n","plt.imshow(figure,cmap='gnuplot2')\n","pos = np.arange(image_size/2,image_size*n,image_size)\n","plt.yticks(pos,np.round(grid_y,1))\n","plt.xticks(pos,np.round(grid_x,1))\n","plt.show()\n","grid = norm.ppf(np.linspace(0.000005, 0.999995, n)) #en los extremos del intervalo de confianza\n","```\n","\n","> h) Experimente y comente si mejora o empeora el desempeño de clasificación de la representación encontrada al aumentar la dimensionalidad de las variables latentes $z$, contrarrestándolo con el AE tradicional ($d' = 2,8,16,32 $). Explique.\n","\n","\n","### 4.3 Variational Autoencoder categórico\n","\n","En esta última sección se explorará el caso en que se cambia el modelamiento sobre la variable latente a una distribuida Multinomial para representar una variable **categórica** que podría entregarnos cierta intuición de capturar las clases del problema de manera no supervisada. Para éste objetivo definiremos el número de variables latentes iguales a la cantidad de clases que sospechamos (en este caso son conocidas y corresponden a 10 dígitos).\n","\n","\n","> a) En primer lugar deberá definir la arquitectura realizando unos cambios leves a la presentada anteriormente. Comente las diferencias sobre los parámetros obtenidos.\n","\n","> El primer cambio es en la distribución obtenida en el encoder.\n","```python\n","... #traditional VAE code here\n","latent_dim = 10\n","logits_z = Dense(latent_dim,activation='linear')(hidden) #log(p(z))\n","encoder = Model(x, logits_z) # build a model to project inputs on the latent space\n","```\n","\n","> Luego, con el decoder creado (igual al caso anterior) es necesario cambiar la forma en que se conectan, ya que el muestreo ahora será a través de un truco diferente para variables categoricas (**Gumbel-Softmax[[10]](#refs))**.\n","```python\n","def sample_gumbel(shape,eps=K.epsilon()):\n","    \"\"\"Inverse Sample function from Gumbel(0, 1)\"\"\"\n","    U = K.random_uniform(shape, 0, 1)\n","    return - K.log( -K.log(U + eps) + eps)\n","def sampling(logits_z):\n","    \"\"\" Perform a Gumbel-Softmax sampling\"\"\"\n","    tau = K.variable(2/3, name=\"temperature\") \n","    z = logits_z + sample_gumbel(K.shape(logits_z)) # logits + gumbel noise\n","    return keras.activations.softmax( z/tau )    \n","from keras.layers import Lambda\n","z = Lambda(sampling, output_shape=(latent_dim,))(logits_z)\n","... #traditional VAE code here\n","vae_norm = Model(x, x_decoded_mean_squash) # instantiate VAE model\n","vae_norm.summary()\n","```\n","\n","> Finalmente la función de pérdida KL cambia ya que se asume un *prior* Multinomial con probabilidad uniforme, $p_{\\theta}(z) = \\frac{1}{K}$. ¿Qué interpretación se le da a este regularizador?\n","```python\n","... #traditional VAE code here\n","dist =  keras.activations.softmax(logits_z) # =p(z)\n","dist_neg_entropy = K.sum(dist * K.log(dist + K.epsilon()), axis=1)\n","kl_disc_loss =  np.log(latent_dim) + dist_neg_entropy #discrete KL-loss\n","vae_loss = K.mean(reconstruction_loss + kl_disc_loss)\n","vae_norm.add_loss(vae_loss)\n","... #traditional VAE code here\n","```\n","\n","> b) Entrene el VAE categórico de la misma manera que realizó con el VAE tradicional en (e) ¿Nota algún cambio en este paso?.\n","\n","> c) Para ver la efectividad del encoder en lograr capturar las clases es necesario medir una métrica de desempeño, sin embargo, las métricas clásicas como *accuracy* o *f1 score* no corresponderían a este caso debido a que las categoría capturada por el encoder no debería estar en el mismo orden de la clase real, ya que fue un entrenamiento no supervisado ¿Cómo se podría cambiar ésto?. Con esto en mente mida alguna métrica de __[*clustering*](https://scikit-learn.org/stable/modules/clustering.html)__ [[11]](#refs) sobre las categorías inferidas por el VAE. Comente.  \n","*Recuerde que se predice los logits de la probabilidad*\n","```python\n","def softmax(x):\n","    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n","    e_x = np.exp(x - x.max(axis=-1,keepdims=True) )\n","    return e_x / e_x.sum(axis=-1, keepdims=True)\n","p_z_train = softmax(encoder.predict(x_train))\n","p_z_test = softmax(encoder.predict(x_test))\n","y_train_pred = p_z_train.argmax(axis=-1)\n","y_test_pred = p_z_test.argmax(axis=-1)\n","...#Example\n","from sklearn.metrics import normalized_mutual_info_score\n","print(normalized_mutual_info_score(y_train, y_train_pred))\n","print(normalized_mutual_info_score(y_test, y_test_pred))\n","```\n","\n","> d) Para entender mejor las categorías inferidas por el VAE genere datos \"activando\" una categoría y luego realizando un *forward pass* sobre el decoder/generador. Comente cualitativamente.\n","```python\n","decoder_input = Input(shape=(latent_dim,))\n","_hid_decoded = decoder_hid(decoder_input)\n","_up_decoded = decoder_upsample(_hid_decoded)\n","_reshape_decoded = decoder_reshape(_up_decoded)\n","_deconv_1_decoded = decoder_deconv_1(_reshape_decoded)\n","_x_decoded_relu = decoder_deconv_2(_deconv_1_decoded)\n","_x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu)\n","generator = Model(decoder_input, _x_decoded_mean_squash) \n","predictions =np.zeros((img_cols * 1 ,img_cols* latent_dim))\n","for i in range(latent_dim):\n","    activate_aux = np.zeros((1,10))\n","    activate_aux[:,i] = 1 #activate a class\n","    predictions[:,i * img_cols:(i + 1) * img_cols] = np.squeeze(generator.predict(activate_aux))\n","import matplotlib.pyplot as plt\n","plt.figure(figsize=(10, 10))\n","plt.imshow(predictions, cmap='gnuplot2')\n","pos = np.arange(img_cols/2, img_cols*latent_dim, img_cols)\n","plt.xticks(pos,range(latent_dim))\n","plt.show()\n","```\n","\n","> e) Genere algunas imágenes aleatorias, comente cualitativamente con lo obtenido con el VAE tradicional ¿Cuál pareciera ser mejor para generar datos? ¿Por qué?"]}]}
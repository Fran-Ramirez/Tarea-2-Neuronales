{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pregunta 1","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0Y1tHj3DNqhX"},"source":["<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n","\n","\n","<hr style=\"height:2px;border:none\"/>\n","<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n","\n","<H3 align='center'> Tarea 2 - Aplicaciones Recientes de Redes Neuronales </H3>\n","\n","<H2 align='center'> Pregunta 1. RNN sobre texto</H2>\n","<H3 align='center'> Francisca Ramírez</H3>\n","<H3 align='center'> Sebastian Ramírez</H3>\n","\n","<hr style=\"height:2px;border:none\"/>\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"84-hf1CuNqhZ"},"source":["<a id=\"primero\"></a>\n","## 1. RNN sobre texto\n","---\n","\n","Hoy en dı́a, una aplicación relevante de las redes neuronales recurrentes es el modelamiento de texto y lenguaje natural. En esta sección abordaremos el problema de procesar sentencias de texto, proporcionadas por GMB (*Groningen Meaning Bank*), para reconocimiento de entidades y *tagger*. En específico, trabajaremos con el dataset proprocionado a través de __[Kaggle](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)__, que está compuesto por más de un millón de palabras, a fin de realizar predicciones sobre distintas tareas de redes recurrentes.\n","\n","<img src=\"https://i.stack.imgur.com/b4sus.jpg\" width=\"70%\" />\n","\n","\n","Descargue los datos de la página de __[Kaggle](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)__ y cárguelos mediante *pandas*.\n","```python\n","import numpy as np\n","import pandas as pd\n","df_ner = pd.read_csv(\"./entity-annotated-corpus/ner.csv\", encoding =\"cp1252\", error_bad_lines=False)\n","df_ner.dropna(inplace=True)\n","```\n","\n","\n","> a) En esta primera instancia trabajaremos con la tarea de realizar un NER *tag* (**Named Entity Recognition**) sobre cada una de las palabras en las sentencias que se nos presenta en los datos. Esta tarea es del tipo *many to many*, es decir, la entrada es una secuencia y la salida es una secuencia, sin *shift*, por lo que necesitaremos una estructura de red adecuada a ésto. En primer lugar extraiga las columnas que utilizaremos del dataset **¿Por qué es conveniente utilizar *lemma* en vez de la misma palabra?**\n","```python\n","dataset = df_ner.loc[:,[\"lemma\",\"word\",\"pos\",\"tag\",\"prev-iob\"]]\n","```\n","Luego de esto cree una estructura que contendrá todas las sentencias u oraciones (lista de *lemmas*) y otra estructura que contendrá las etiquetas (lista de *tags*). **¿Cuales son las dimensiones de ambas estructuras? ¿Cada dato de ejemplo tiene las mismas dimensiones que el resto?**\n","```python\n","n_used = 500000 #data to use-- your choice\n","dataX_raw,dataY_raw = [],[]\n","lemmas,labels = set(), set()  #uniques\n","for fila in dataset.values[:n_used]:\n","    if fila[-1]==\"__START1__\": \n","        dataX_raw.append(sentence)\n","        dataY_raw.append(labels_sentence)\n","        sentence= []\n","        labels_sentence = []\n","    lemmas.add(fila[0])\n","    labels.add(fila[3])\n","    sentence.append(fila[0]) #add lemma\n","    labels_sentence.append(fila[3]) #TAG\n","dataX_raw = dataX_raw[1:]\n","dataY_raw = dataY_raw[1:]\n","```    \n","\n","> b) Estudie la distribución del largo de los textos a procesar. Estudie también la frecuencia con la que aparecen las palabras en todo el dataset. **¿Se observa una ley Zipf?**[[1]](#refs) Realice un gráfico de la cantidad de datos por clase. Comente.\n","\n","\n","> c) Para representar cada posible *tag* y *lemma* de modo que la red pueda manejarlo, será necesario codificarlos a un número único (*indice*) ¿Cuántos *tags* y *lemmas* distintos existen?  Comente sobre el significado del *tag* para cada *lemma*. **Finalmente mida el largo máximo de entre todas las sentencias**.\n","```python\n","n_labels = len(labels)\n","lab2idx = {t: i for i, t in enumerate(labels)}\n","dataY = [[lab2idx[ner] for ner in ner_tags ] for ner_tags in dataY_raw] #Converting tags to indexs\n","n_lemmas = len(lemmas)\n","lemma2idx = {w: i for i, w in enumerate(lemmas)} \n","dataX = [[lemma2idx[lemma] for lemma in sentence ] for sentence in dataX_raw] #Converting text to indexs\n","```\n","\n","> d) Debido a la distinta extensión de textos se deberá **realizar *padding* para estandarizar el largo**,\n","considere algun carácter especial **no presente en el vocabulario** para codificar el espacio en blanco en ambos (entrada y salida), por ejemplo si el largo máximo es de 4 y se tiene la sentencia \"the rocket\" codificada como [32,4] será necesario agregar un *lemma* que codificado significará el fin de la sentencia \"the rocket *ENDPAD ENDPAD*\" que codificado quedará como [32,4,*N, N*]. Decida, respecto al cómo funciona una red recurrente y su *memoria*, sobre qué le parece más conveniente al momento de rellenar con un valor especial ¿Al principio o al final de la sentencia? Comente\n","```python\n","lemma2idx[\"END\"] = n_lemmas #add fullfill lemma and tag to the dictionary\n","lab2idx[\"END\"] = n_labels\n","n_labels +=1\n","n_lemmas +=1\n","from keras.preprocessing import sequence\n","X = sequence.pad_sequences(dataX, maxlen=max_input_lenght,padding='pre' or 'post',value=lemma2idx[\"yourspecialcharacter\"])\n","y = sequence.pad_sequences(dataY, maxlen=max_input_lenght,padding='pre' or 'post',value=lab2idx[\"endtagger\"])\n","del dataY[:],dataX[:]\n","```\n","\n","> e) Para poder generar una representación adecuada sobre los datos de entrada que permita realizar operaciones lineales, deberá generar una representación a un vector denso. Para ésto se utilizará la arquitectura de autoencoder **Word2Vec** [[2]](#refs) sobre textos *raws* de largo variable, en donde el *encoder* codifica una palabra categórica (*target*) a un vector denso de dimensionalidad $d$ mientras que el *decoder* genera palabras en el contexto (*context*) de la palabra *target* (en una vecindad alrededor). La idea detrás es que palabras similares sean proyectadas a una región cercana en el espacio de *embedding* ¿Cuál es la importancia del parámetro min_count? ¿Cuántos *lemmas* ve Word2Vec?\n","```python\n","from gensim.models import Word2Vec\n","EMBEDDING_DIM = 32\n","window_size = 5\n","nb_epoch = 5\n","batch_size = 6000\n","min_count = 3\n","model = Word2Vec(dataX_raw,size=EMBEDDING_DIM,window=window_size,batch_words=batch_size,iter=nb_epoch,\n","                 min_count=min_count, negative=5,sg=1) #sg=1 mean skip-gram\n","embeddings_index = {vocab_word: model.wv[vocab_word] for vocab_word in model.wv.vocab}\n","len(embeddings_index.keys())\n","```\n","Genere una matriz de *embeddings* que se utilizarán como capa neuronal.\n","```python\n","embedding_matrix = np.zeros((n_lemmas, EMBEDDING_DIM))\n","for word, i in lemma2idx.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None: #if word does not has embedding\n","        embedding_matrix[i] = embedding_vector\n","```\n","Luego, para poder realizar una clasificación sobre los datos en la salida será necesario representarlos *one hot vectors*, esto resultará en un arreglo tridimensional.\n","```python\n","from keras.utils import to_categorical\n","y = np.asarray([to_categorical(i, num_classes=n_labels) for i in y])\n","```\n","\n","> f) Luego de esto cree los conjuntos de entrenamiento y de prueba con el código a continuación **¿Cuáles son las dimensiones de entrada y salida de cada conjunto?** Comente\n","```python\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=22)\n","y_train.shape\n","```\n","\n","> g) Defina una red neuronal recurrente *many to many* con compuertas LSTM para aprender a *tagear* la entidad en el texto. Esta red debe procesar la secuencia de *lemmas* rellenados (o sin rellenar) y entregar el *tag* a cada uno de los *lemmas*, por lo que la salida de la red es una por cada instante de tiempo que se necesita entregar un *output*. La primera capa de la red a construir debe tener los vectores de *embedding* encontrados por **Word2Vec**. **Comente sobre los cambios que sufre un dato al ingresar a la red y la cantidad de parámetros de la red**. Entrene y luego evalúe su desempeño sobre ambos conjuntos. \n","```python\n","from keras.models import Sequential\n","from keras.layers import LSTM, Embedding, Dense, Dropout,TimeDistributed\n","model = Sequential()\n","model.add(Embedding(input_dim=n_lemmas, output_dim=EMBEDDING_DIM, input_length=max_input_lenght,\n","                    trainable=False, weights = [embedding_matrix]))\n","model.add(LSTM(units=100,return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(TimeDistributed(Dense(n_labels, activation='softmax')))\n","model.summary()\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=128)\n","```\n","Para evaluar su modelo utilice una métrica adecuada para el desbalance presente entre las clases como identificó en el punto b). Tenga presente en este punto el **no evaluar** la clase/símbolo que añadió para realizar *padding* a los *tag*.  \n","*Hint: podría \"truncar\" la salida predicha hasta el largo real de esa sentencia*.\n","```python\n","from sklearn.metrics import f1_score\n","dataY_pred = model.predict_classes(X_test,verbose=0) #process... to remove prediction on \"endtagger\" symbol\n","f1_score_bydata = [f1_score(true, pred ,average='macro') for true,pred in zip(dataY,dataY_pred) ]\n","print(\"F1 score on test: \", np.mean(f1_score_bydata) )\n","```\n","\n","> h) Varíe con seguir entrenando la capa de embedding seteada al definir la arquitectura, ésto es cambiar a *trainable=True*, compare el desempeño y el número de parámetros (entrenables) con lo anterior. Comente\n","\n","> i) Experimente con cambiar la *gate* de recurrencia a una con menos parámetros pero que mantiene la capacidad de memoria de la LSTM, ésta es la compuerta GRU. Comente sobre los resultados esperados y observados.\n","```python\n","from keras.layers import GRU\n","...\n","model.add(GRU(units=100,return_sequences=True))\n","...\n","```\n","\n","> j) Algunos autores señalan la importante dependencia que existe en texto, no solo con las palabras anteriores, sino que con las que siguen. Mejore la red utilizando una red neuronal recurrente Bidireccional, es decir, con recurrencia en ambas direcciones sobre la secuencia de *lemmas*. Comente cuál debiera ser la forma correcta de usar el parámetro merge_mode (concatenar, multiplicar, sumar o promediar) para este caso. Además comente las transformaciones que sufre el patrón de entrada al pasar por las capas. ¿Mejora o empeora el desempeño? Analice.\n","```python\n","from keras.layers import Bidirectional\n","...\n","gate_layer = (LSTM or GRU )(units=100,return_sequences=True)\n","model.add(Bidirectional(gate_layer, merge_mode=choose))\n","...\n","```\n","\n","> k) En base a lo experimentado, **mejore el desempeño de las redes encontradas**, ya sea utilizando y/o combinando las distintas variaciones que se hicieron en los distintos ítemes, como bien alguna mejora en el pre-proceso de los datos (largo de secuencia, el tipo de *padding* o alguna otra), agregar mayor profundidad, variar el número de unidades/neuronas, utilizando otra *gate* de recurrencia (en https://keras.io/layers/recurrent/), cambiar los vectores de *embedding* por unos entrenados en otros dataset más grandes (https://nlp.stanford.edu/projects/glove/), entre otros.\n","\n","\n","> l) Utilice la red con mejor desempeño encontrada, idealmente la encontrada en (j), y **muestre las predicciones** del *NER tager*, sobre algún ejemplo de pruebas, comente.  \n","```python\n","p = model.predict(np.array([X_test[i]]))\n","p = np.argmax(p, axis=-1)\n","print(\"{:15}: {}\".format(\"Lemma\", \"Pred\"))\n","for w,pred in zip(X_test[i],p[0]):\n","    print(\"{:15}: {}\".format(lemmas[w],labels[pred]))\n","```"]}]}